{
    "@graph": [
        {
            "@id": "gnd:1025829506",
            "sameAs": "Klapuri, Anssi"
        },
        {
            "@id": "gnd:1060018942",
            "sameAs": "Abeßer, Jakob"
        },
        {
            "@id": "gnd:123175844",
            "sameAs": "Müller, Meinard"
        },
        {
            "@id": "gnd:13763398X",
            "sameAs": "Schuller, Gerald"
        },
        {
            "@id": "gnd:4072803-1",
            "sameAs": "Information Retrieval"
        },
        {
            "@id": "gnd:4120775-0",
            "sameAs": "Musiksignal"
        },
        {
            "@id": "gnd:4191522-7",
            "sameAs": "Tonsignal"
        },
        {
            "@id": "gnd:4261650-5",
            "sameAs": "Elektrobass"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A798977809",
            "@type": "bibo:Thesis",
            "P1053": "Online-Ressource (PDF-Datei: XI, 168 S., 4,92 MB)",
            "description": "Ill., graph. Darst.",
            "identifier": [
                "(firstid)GBV:798977809",
                "(ppn)798977809"
            ],
            "publisher": "Univ.-Bibliothek",
            "subject": [
                "(classificationName=linseach:mapping)inf",
                "(classificationName=linseach:mapping)pae",
                "(classificationName=linseach:mapping)elt",
                "(classificationName=ddc-dbn)621.3",
                "(classificationName=ddc-dbn)780",
                "(classificationName=ddc-dbn)004",
                "(classificationName=linseach:mapping)phy"
            ],
            "title": "Automatic transcription of bass guitar tracks applied for music genre classification and sound synthesis",
            "abstract": "Musiksignale bestehen in der Regel aus einer Überlagerung mehrerer Einzelinstrumente. Die meisten existierenden Algorithmen zur automatischen Transkription und Analyse von Musikaufnahmen im Forschungsfeld des Music Information Retrieval (MIR) versuchen, semantische Information direkt aus diesen gemischten Signalen zu extrahieren. In den letzten Jahren wurde häufig beobachtet, dass die Leistungsfähigkeit dieser Algorithmen durch die Signalüberlagerungen und den daraus resultierenden Informationsverlust generell limitiert ist. Ein möglicher  Lösungsansatz besteht darin, mittels Verfahren der Quellentrennung die beteiligten Instrumente vor der Analyse klanglich zu isolieren. Die Leistungsfähigkeit dieser Algorithmen ist zum aktuellen Stand der Technik jedoch nicht immer ausreichend, um eine sehr gute Trennung der Einzelquellen zu ermöglichen. In dieser Arbeit werden daher ausschließlich isolierte Instrumentalaufnahmen untersucht, die klanglich nicht von anderen Instrumenten überlagert sind. Exemplarisch werden anhand der elektrischen Bassgitarre auf die Klangerzeugung dieses Instrumentes hin spezialisierte Analyse- und Klangsynthesealgorithmen entwickelt und evaluiert. Im ersten Teil der vorliegenden Arbeit wird ein Algorithmus vorgestellt, der eine automatische Transkription von Bassgitarrenaufnahmen durchführt. Dabei wird das Audiosignal durch verschiedene Klangereignisse beschrieben, welche den gespielten Noten auf dem Instrument entsprechen. Neben den üblichen Notenparametern Anfang, Dauer, Lautstärke und Tonhöhe werden dabei auch instrumentenspezifische Parameter wie die verwendeten Spieltechniken sowie die Saiten- und Bundlage auf dem Instrument automatisch extrahiert. Evaluationsexperimente anhand zweier neu erstellter Audiodatensätze belegen, dass der vorgestellte Transkriptionsalgorithmus auf einem Datensatz von realistischen Bassgitarrenaufnahmen eine höhere Erkennungsgenauigkeit erreichen kann als drei existierende Algorithmen aus dem Stand der Technik. Die Schätzung der instrumentenspezifischen Parameter kann insbesondere für isolierte Einzelnoten mit einer hohen Güte durchgeführt werden. Im zweiten Teil der Arbeit wird untersucht, wie aus einer Notendarstellung typischer sich wiederholender Basslinien auf das Musikgenre geschlossen werden kann. Dabei werden Audiomerkmale extrahiert, welche verschiedene tonale, rhythmische, und strukturelle Eigenschaften von Basslinien quantitativ beschreiben. Mit Hilfe eines neu erstellten Datensatzes von 520 typischen Basslinien aus 13 verschiedenen Musikgenres wurden drei verschiedene Ansätze für die automatische Genreklassifikation verglichen. Dabei zeigte sich, dass mit Hilfe eines regelbasierten Klassifikationsverfahrens nur Anhand der Analyse der Basslinie eines Musikstückes bereits eine mittlere Erkennungsrate von 64,8 % erreicht werden konnte. Die Re-synthese der originalen Bassspuren basierend auf den extrahierten Notenparametern wird im dritten Teil der Arbeit untersucht. Dabei wird ein neuer Audiosynthesealgorithmus vorgestellt, der basierend auf dem Prinzip des Physical Modeling verschiedene Aspekte der für die Bassgitarre charakteristische Klangerzeugung wie Saitenanregung, Dämpfung, Kollision zwischen Saite und Bund sowie dem Tonabnehmerverhalten nachbildet. Weiterhin wird ein parametrischerAudiokodierungsansatz diskutiert, der es erlaubt, Bassgitarrenspuren nur anhand der ermittelten notenweisen Parameter zu übertragen um sie auf Dekoderseite wieder zu resynthetisieren. Die Ergebnisse mehrerer Hötest belegen, dass der vorgeschlagene Synthesealgorithmus eine Re- Synthese von Bassgitarrenaufnahmen mit einer besseren Klangqualität ermöglicht als die Übertragung der Audiodaten mit existierenden Audiokodierungsverfahren, die auf sehr geringe Bitraten ein gestellt sind.",
            "contributor": [
                "gnd:123175844",
                "gnd:1025829506",
                "gnd:13763398X"
            ],
            "dcterms:contributor": "Technische Informationsbibliothek (TIB)",
            "creator": "gnd:1060018942",
            "isPartOf": "(collectioncode)GBV-ODiss",
            "issued": "2014",
            "language": "http://id.loc.gov/vocabulary/iso639-1/en",
            "license": "open access",
            "medium": "rda:termList/RDACarrierType/1018",
            "dcterms:subject": [
                {
                    "@id": "gnd:4120775-0"
                },
                {
                    "@id": "gnd:4261650-5"
                },
                {
                    "@id": "gnd:4191522-7"
                },
                {
                    "@id": "gnd:4072803-1"
                }
            ]
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "contributor": {
            "@id": "http://purl.org/dc/terms/contributor",
            "@type": "@id"
        },
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "title": "http://purl.org/dc/elements/1.1/title",
        "license": "http://purl.org/dc/terms/license",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "issued": "http://purl.org/dc/terms/issued",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "abstract": "http://purl.org/dc/terms/abstract",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "description": "http://purl.org/dc/elements/1.1/description",
        "publisher": "http://purl.org/dc/elements/1.1/publisher",
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}