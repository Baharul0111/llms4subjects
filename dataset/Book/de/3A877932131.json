{
    "@graph": [
        {
            "@id": "gnd:1053433689",
            "sameAs": "Python 3.4"
        },
        {
            "@id": "gnd:1080537872",
            "sameAs": "Raschka, Sebastian"
        },
        {
            "@id": "gnd:1113598565",
            "sameAs": "Python 3.5"
        },
        {
            "@id": "gnd:4123037-1",
            "sameAs": "Datenanalyse"
        },
        {
            "@id": "gnd:4193754-5",
            "sameAs": "Maschinelles Lernen"
        },
        {
            "@id": "gnd:4802620-7",
            "sameAs": "Big Data"
        },
        {
            "@id": "https://www.tib.eu/de/suchen/id/TIBKAT%3A877932131",
            "@type": "bibo:Book",
            "P1053": "1 Online-Ressource (424 Seiten)",
            "description": "Illustrationen, Diagramme",
            "identifier": [
                "(isbn13)9783958454224",
                "(ppn)877932131",
                "(isbn13)9783958454248",
                "(firstid)GBV:877932131",
                "(isbn)3958454224"
            ],
            "publisher": "mitp",
            "subject": [
                "Electronic books",
                "(classificationName=ddc)006.31",
                "(classificationName=ddc-dbn)004",
                "Python (Computer program language)",
                "(classificationName=linseach:mapping)inf"
            ],
            "title": "Machine Learning mit Python : das Praxis-Handbuch für Data Science, Predictive Analytics und Deep Learning",
            "abstract": [
                "Cover -- Titel -- Impressum -- Inhaltsverzeichnis -- Vorwort -- Über den Autor -- Danksagungen -- Über die Korrektoren -- Einleitung -- Kapitel 1: Wie Computer aus Daten lernen können -- 1.1 Intelligente Maschinen, die Daten in Wissen verwandeln -- 1.2 Die drei Arten des Machine Learnings -- 1.2.1 Mit überwachtem Lernen Vorhersagen treffen -- 1.2.2 Interaktive Aufgaben durch verstärkendes Lernen lösen -- 1.2.3 Durch unüberwachtes Lernen verborgene Strukturen erkennen -- 1.3 Grundlegende Terminologie und Notation -- 1.4 Entwicklung eines Systems für das Machine Learning -- 1.4.1 Vorverarbeitung: Daten in Form bringen -- 1.4.2 Trainieren und Auswählen eines Vorhersagemodells -- 1.4.3 Bewertung von Modellen und Vorhersage anhand unbekannter Dateninstanzen -- 1.5 Machine Learning mit Python -- 1.5.1 Python-Pakete installieren -- 1.6 Zusammenfassung -- Kapitel 2: Lernalgorithmen für die Klassifizierung trainieren -- 2.1 Künstliche Neuronen: Ein kurzer Blick auf die Anfänge des Machine Learnings -- 2.2 Implementierung eines Perzeptron-Lernalgorithmus in Python -- 2.2.1 Trainieren eines Perzeptron-Modells auf die Iris-Datensammlung -- 2.3 Adaptive lineare Neuronen und die Konvergenz des Lernens -- 2.3.1 Straffunktionen mit dem Gradientenabstiegsverfahren minimieren -- 2.3.2 Implementierung eines adaptiven linearen Neurons in Python -- 2.3.3 Large-scale Machine Learning und stochastisches Gradientenabstiegsverfahren -- 2.4 Zusammenfassung -- Kapitel 3: Machine-Learning-Klassifizierer mit scikit-learn verwenden -- 3.1 Auswahl eines Klassifizierungsalgorithmus -- 3.2 Erste Schritte mit scikit-learn -- 3.2.1 Trainieren eines Perzeptrons mit scikit-learn -- 3.3 Klassenwahrscheinlichkeiten durch logistische Regression modellieren -- 3.3.1 Logistische Regression und bedingte Wahrscheinlichkeiten -- 3.3.2 Gewichtungen der logistischen Straffunktion ermitteln.",
                "5.2 Überwachte Datenkomprimierung durch lineare Diskriminanzanalyse -- 5.2.1 Berechnung der Streumatrizen -- 5.2.2 Auswahl linearer Diskriminanten für den neuen Merkmalsunterraum -- 5.2.3 Projektion in den neuen Merkmalsraum -- 5.2.4 LDA mit scikit-learn -- 5.3 Kernel-Hauptkomponentenanalyse für nichtlineare Zuordnungen verwenden -- 5.3.1 Kernel-Funktionen und der Kernel-Trick -- 5.3.2 Implementierung einer Kernel-Hauptkomponentenanalyse in Python -- 5.3.3 Projizieren neuer Datenpunkte -- 5.3.4 Kernel-Hauptkomponentenanalyse mit scikit-learn -- 5.4 Zusammenfassung -- Kapitel 6: Bewährte Verfahren zur Modellbewertung und Hyperparameter-Abstimmung -- 6.1 Arbeitsabläufe mit Pipelines optimieren -- 6.1.1 Die Wisconsin-Brustkrebs-Datensammlung -- 6.1.2 Transformer und Schätzer in einer Pipeline kombinieren -- 6.2 Beurteilung des Modells durch k-fache Kreuzvalidierung -- 6.2.1 2-fache Kreuzvalidierung -- 6.2.2 k-fache Kreuzvalidierung -- 6.3 Algorithmen mit Lern- und Validierungskurven debuggen -- 6.3.1 Probleme mit Bias und Varianz anhand von Lernkurven erkennen -- 6.3.2 Überanpassung und Unteranpassung anhand von Validierungskurven erkennen -- 6.4 Feinabstimmung eines Lernmodells durch Rastersuche -- 6.4.1 Hyperparameterabstimmung durch Rastersuche -- 6.4.2 Algorithmenauswahl durch verschachtelte Kreuzvalidierung -- 6.5 Verschiedene Kriterien zur Leistungsbewertung -- 6.5.1 Interpretation einer Wahrheitsmatrix -- 6.5.2 Optimierung der Genauigkeit und der Trefferquote eines Klassifizierungsmodells -- 6.5.3 Receiver-Operating-Characteristic-Diagramme -- 6.5.4 Bewertungskriterien für Mehrfachklassifizierungen -- 6.6 Zusammenfassung -- Kapitel 7: Kombination verschiedener Modelle für das Ensemble Learning -- 7.1 Ensemble Learning -- 7.2 Implementierung eines einfachen Mehrheitsentscheidungs-Klassifizierers",
                "7.2.1 Kombination mehrerer Klassifizierungsalgorithmen per Mehrheitsentscheidung -- 7.3 Bewertung und Abstimmung des Klassifizierer-Ensembles -- 7.4 Bagging: Klassifizierer-Ensembles anhand von Bootstrap-Stichproben entwickeln -- 7.5 Schwache Klassifizierer durch adaptives Boosting verbessern -- 7.6 Zusammenfassung -- Kapitel 8: Machine Learning zur Analyse von Stimmungslagen nutzen -- 8.1 Die IMDb-Filmdatenbank -- 8.2 Das Bag-of-words-Modell -- 8.2.1 Wörter in Merkmalsvektoren umwandeln -- 8.2.2 Beurteilung der Wortrelevanz durch das Tf-idf-Maß -- 8.2.3 Textdaten bereinigen -- 8.2.4 Dokumente in Token zerlegen -- 8.3 Ein logistisches Regressionsmodell für die Dokumentklassifizierung trainieren -- 8.4 Verarbeitung großer Datenmengen: Online-Algorithmen und Out-of-Core Learning -- 8.5 Zusammenfassung -- Kapitel 9: Einbettung eines Machine-Learning-Modells in eine Webanwendung -- 9.1 Serialisierung angepasster Schätzer mit scikit-learn -- 9.2 Einrichtung einer SQLite-Datenbank zum Speichern von Daten -- 9.3 Entwicklung einer Webanwendung mit Flask -- 9.3.1 Die erste Webanwendung mit Flask -- 9.3.2 Formularvalidierung und -ausgabe -- 9.4 Der Filmbewertungsklassifizierer als Webanwendung -- 9.5 Einrichtung der Webanwendung auf einem öffentlich zugänglichen Webserver -- 9.5.1 Updaten des Filmbewertungsklassifizierers -- 9.6 Zusammenfassung -- Kapitel 10: Vorhersage stetiger Zielvariablen durch Regressionsanalyse -- 10.1 Ein einfaches lineares Regressionsmodell -- 10.2 Die Lebensbedingungen-Datensammlung -- 10.2.1 Visualisierung der wichtigen Eigenschaften einer Datenmenge -- 10.3 Implementierung eines linearen Regressionsmodells mit der Methode der kleinsten Quadrate -- 10.3.1 Berechnung der Regressionsparameter mit dem Gradientenabstiegsverfahren -- 10.3.2 Abschätzung der Koeffizienten eines Regressionsmodells mit scikit-learn",
                "10.4 Anpassung eines robusten Regressionsmodells mit dem RANSAC-Algorithmus -- 10.5 Bewertung der Leistung linearer Regressionsmodelle -- 10.6 Regularisierungsverfahren für die Regression einsetzen -- 10.7 Polynomiale Regression: Umwandeln einer linearen Regression in eine Kurve -- 10.7.1 Modellierung nichtlinearer Zusammenhänge in der Lebensbedingungen-Datensammlung -- 10.7.2 Handhabung nichtlinearer Beziehungen mit Random Forests -- 10.8 Zusammenfassung -- Kapitel 11: Verwendung nicht gekennzeichneter Daten: Clusteranalyse -- 11.1 Gruppierung von Objekten nach Ähnlichkeit mit dem k-Means-Algorithmus -- 11.1.1 Der k-Means++-Algorithmus -- 11.1.2 »Harte« und »weiche« Clustering-Algorithmen -- 11.1.3 Die optimale Anzahl der Cluster mit dem Ellenbogenkriterium ermitteln -- 11.1.4 Quantifizierung der Clustering-Güte mit Silhouettendiagrammen -- 11.2 Cluster als hierarchischen Baum organisieren -- 11.2.1 Hierarchisches Clustering einer Distanzmatrix -- 11.2.2 Dendrogramme und Heatmaps verknüpfen -- 11.2.3 Agglomeratives Clustering mit scikit-learn -- 11.3 Bereiche hoher Dichte mit DBSCAN ermitteln -- 11.4 Zusammenfassung -- Kapitel 12: Künstliche neuronale Netze für die Bilderkennung trainieren -- 12.1 Modellierung komplexer Funktionen mit künstlichen neuronalen Netzen -- 12.1.1 Einschichtige neuronale Netze -- 12.1.2 Mehrschichtige neuronale Netzarchitektur -- 12.1.3 Aktivierung eines neuronalen Netzes durch Vorwärtspropagation -- 12.2 Klassifizierung handgeschriebener Ziffern -- 12.2.1 Die MNIST-Datensammlung -- 12.2.2 Implementierung eines mehrschichtigen Perzeptrons -- 12.3 Trainieren eines künstlichen neuronalen Netzes -- 12.3.1 Berechnung der logistischen Straffunktion -- 12.3.2 Trainieren neuronaler Netze durch Backpropagation -- 12.4 Ein Gespür für die Backpropagation entwickeln -- 12.5 Debugging neuronaler Netze durch Gradientenprüfung",
                "3.3.3 Trainieren eines logistischen Regressionsmodells mit scikit-learn -- 3.3.4 Überanpassung durch Regularisierung verhindern -- 3.4 Maximum-Margin-Klassifizierung mit Support Vector Machines -- 3.4.1 Maximierung des Randbereichs -- 3.4.2 Handhabung des nicht linear trennbaren Falls mit Schlupfvariablen -- 3.4.3 Alternative Implementierungen in scikit-learn -- 3.5 Nichtlineare Aufgaben mit einer Kernel-SVM lösen -- 3.5.1 Mit dem Kernel-Trick Hyperebenen in höherdimensionalen Räumen finden -- 3.6 Lernen mit Entscheidungsbäumen -- 3.6.1 Maximierung des Informationsgewinns: Daten ausreizen -- 3.6.2 Konstruktion eines Entscheidungsbaums -- 3.6.3 Schwache Klassifizierer mit Random Forests zu besseren kombinieren -- 3.7 k-Nearest-Neighbor: Ein Lazy-Learning-Algorithmus -- 3.8 Zusammenfassung -- Kapitel 4: Gut geeignete Trainingsdatenmengen: Datenvorverarbeitung -- 4.1 Umgang mit fehlenden Daten -- 4.1.1 Exemplare oder Merkmale mit fehlenden Werten entfernen -- 4.1.2 Fehlende Werte ergänzen -- 4.1.3 Die Schätzer-API von scikit-learn -- 4.2 Handhabung kategorialer Daten -- 4.2.1 Zuweisung von ordinalen Merkmalen -- 4.2.2 Kodierung der Klassenbezeichnungen -- 4.2.3 One-hot-Kodierung der nominalen Merkmale -- 4.3 Aufteilung einer Datensammlung in Trainings- und Testdaten -- 4.4 Anpassung der Merkmale -- 4.5 Auswahl aussagekräftiger Merkmale -- 4.5.1 Dünnbesetzte Lösungen durch L1-Regularisierung -- 4.5.2 Algorithmen zur sequenziellen Auswahl von Merkmalen -- 4.6 Beurteilung der Bedeutung von Merkmalen mit Random Forests -- 4.7 Zusammenfassung -- Kapitel 5: Datenkomprimierung durch Dimensionsreduktion -- 5.1 Unüberwachte Dimensionsreduktion durch Hauptkomponentenanalyse -- 5.1.1 Totale Varianz und Varianzaufklärung -- 5.1.2 Merkmalstransformation -- 5.1.3 Hauptkomponentenanalyse mit scikit-learn",
                "12.6 Konvergenz in neuronalen Netzen"
            ],
            "alternative": "Python machine learning",
            "contributor": "Technische Informationsbibliothek (TIB)",
            "creator": "gnd:1080537872",
            "isPartOf": [
                "(collectioncode)BSZ-30-PQE-S2AAFH",
                "(collectioncode)ZDB-89-EBL",
                "(collectioncode)ZDB-30-PQE"
            ],
            "issued": "2017",
            "language": "http://id.loc.gov/vocabulary/iso639-1/de",
            "license": "commercial licence",
            "medium": "rda:termList/RDACarrierType/1018",
            "dcterms:subject": [
                {
                    "@id": "gnd:1053433689"
                },
                {
                    "@id": "gnd:4193754-5"
                },
                {
                    "@id": "gnd:4802620-7"
                },
                {
                    "@id": "gnd:1113598565"
                },
                {
                    "@id": "gnd:4123037-1"
                }
            ],
            "tableOfContents": "http://d-nb.info/1096181851/04",
            "P30128": "mitp Professional",
            "P60163": "Frechen"
        }
    ],
    "@id": "urn:x-arq:DefaultGraphNode",
    "@context": {
        "sameAs": "http://www.w3.org/2002/07/owl#sameAs",
        "subject": "http://purl.org/dc/elements/1.1/subject",
        "abstract": "http://purl.org/dc/terms/abstract",
        "identifier": "http://purl.org/dc/elements/1.1/identifier",
        "issued": "http://purl.org/dc/terms/issued",
        "isPartOf": "http://purl.org/dc/terms/isPartOf",
        "P60163": "http://www.rdaregistry.info/Elements/u/#P60163",
        "P1053": "http://iflastandards.info/ns/isbd/elements/P1053",
        "title": "http://purl.org/dc/elements/1.1/title",
        "language": {
            "@id": "http://purl.org/dc/terms/language",
            "@type": "@id"
        },
        "contributor": "http://purl.org/dc/terms/contributor",
        "publisher": "http://purl.org/dc/elements/1.1/publisher",
        "P30128": "http://www.rdaregistry.info/Elements/m/#P30128",
        "license": "http://purl.org/dc/terms/license",
        "creator": {
            "@id": "http://purl.org/dc/terms/creator",
            "@type": "@id"
        },
        "description": "http://purl.org/dc/elements/1.1/description",
        "medium": {
            "@id": "http://purl.org/dc/terms/medium",
            "@type": "@id"
        },
        "alternative": "http://purl.org/dc/terms/alternative",
        "tableOfContents": "http://purl.org/dc/terms/tableOfContents",
        "umbel": "http://umbel.org/umbel#",
        "rdau": "http://www.rdaregistry.info/Elements/u/#",
        "owl": "http://www.w3.org/2002/07/owl#",
        "dcterms": "http://purl.org/dc/terms/",
        "bibo": "http://purl.org/ontology/bibo/",
        "rdam": "http://www.rdaregistry.info/Elements/m/#",
        "gnd": "http://d-nb.info/gnd/",
        "isbd": "http://iflastandards.info/ns/isbd/elements/",
        "rda": "http://rdvocab.info/",
        "doi": "https://doi.org/"
    }
}